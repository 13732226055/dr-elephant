<p>
    This analysis shows how well the number of mappers is adjusted.<br>
    This should allow you to better tweak the number of mappers for your job.<br>
    There are two possible situations that needs some tuning.
</p>
<h3>Too many small files</h3>
<p>
    This usually happens when the Hadoop job has:
<ul>
    <li>A <u>large</u> number of mappers</li>
    <li><u>Small</u> file size</li>
    <li><u>Short</u> mapper runtime</li>
</ul>
</p>
<h5>Example</h5>
<p>
<div class="list-group">
    <a class="list-group-item list-group-item-danger" href="#">
        <h4 class="list-group-item-heading">Mapper Input Size</h4>
        <table class="list-group-item-text table table-condensed left-table">
            <thead><tr><th colspan="2">Severity: Critical</th></tr></thead>
            <tbody>
            <tr>
                <td>Number of tasks</td>
                <td>916</td>
            </tr>
            <tr>
                <td>Average task input size</td>
                <td>19 KB</td>
            </tr>
            <tr>
                <td>Average task runtime</td>
                <td>1min 32s</td>
            </tr>
            </tbody>
        </table>
    </a>
</div>
</p>
<h4>Suggestions</h4>
<p>
    Set the number of mappers smaller by specifying a number or combining small files using Pig or Hive.<br>
    <br>
    For Hadoop/Java jobs: Try increasing mapred.max.split.size in the job conf so that each mapper will now process larger data.<br>
    For Apache-Pig jobs : Try setting pig.maxCombinedSplitSize and mapred.max.split.size to something larger.<br>
    For Apache-Hive jobs: Try increasing mapred.max.split.size in the job conf so that each mapper will now process larger data.<br>
</p>
<h3>Large files/Unsplittable files</h3>
<p>
    This usually happens when the Hadoop job has:
<ul>
    <li>A <u>small</u> number of mappers</li>
    <li><u>Large</u> file size (a few GB's)</li>
    <li><u>Long</u> mapper runtime</li>
</ul>
</p>
<h5>Example</h5>
<p>
<div class="list-group">
    <a class="list-group-item list-group-item-danger" href="#">
        <h4 class="list-group-item-heading">Mapper Input Size</h4>
        <table class="list-group-item-text table table-condensed left-table">
            <thead><tr><th colspan="2">Severity: Critical</th></tr></thead>
            <tbody>
            <tr>
                <td>Number of tasks</td>
                <td>100</td>
            </tr>
            <tr>
                <td>Average task input</td>
                <td>3 GB</td>
            </tr>
            <tr>
                <td>Average task runtime</td>
                <td>32 min 30s</td>
            </tr>
            </tbody>
        </table>
    </a>
</div>
</p>
<h4>Suggestions</h4>
<p>
    Set the number of mappers larger by giving a specific number.<br>
    <br>
    The maximum map split size is controlled by the "mapred.max.split.size" parameter. By decreasing this value
    below dfs.block.size, you can reduce the maximum input size for each mapper, thereby increase the number of mappers
    in your job.<br>
</p>

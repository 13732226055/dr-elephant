<p>
    This analysis shows how well the number of mappers is adjusted.<br>
    This should allow you to better tweak the number of mappers for your job.<br>
    There are two possible situations that needs some tuning.
</p>
<h3>Too many small files</h3>
<p>
    This usually happens when the Hadoop job has:
<ul>
    <li>A <u>large</u> number of mappers</li>
    <li><u>Small</u> file size</li>
    <li><u>Short</u> mapper runtime</li>
</ul>
</p>
<h5>Example</h5>
<p>
<div class="list-group">
    <a class="list-group-item list-group-item-danger" href="#">
        <h4 class="list-group-item-heading">Mapper Input Size</h4>
        <table class="list-group-item-text table table-condensed left-table">
            <thead><tr><th colspan="2">Severity: Critical</th></tr></thead>
            <tbody>
            <tr>
                <td>Number of tasks</td>
                <td>916</td>
            </tr>
            <tr>
                <td>Average task input size</td>
                <td>19 KB</td>
            </tr>
            <tr>
                <td>Average task runtime</td>
                <td>1min 32s</td>
            </tr>
            </tbody>
        </table>
    </a>
</div>
</p>
<h4>Suggestions</h4>
<p>
    Note: By default HadoopJava will not combine small files. if thatâ€™s the case, you should either use CombineFileInputFormat or use Pig/Hive.
   <br>
   <br>
    Otherwise, you should tune mapper split size to reduce number of mappers and let each mapper process larger data <br>
    The parameter for changing split size is: <br>
    <ul>
    <li>mapred.max/min.split.size (Hadoop 1)</li>
    <li>mapreduce.input.fileinputformat.split.minsize/maxsize (Hadoop 2)</li>
    <li>pig.maxCombinedSplitSize (Pig Only)</li>
    </ul>
    Examples on how to set them:
    <ul>
    <li>HadoopJava: conf.setLong("mapred.min.split.size",XXXXX)</li>
    <li>Pig: set mapred.min.split.size XXXXX </li>
    <li>Hive: set mapred.min.split.size=XXXXX</li>
    </ul>

    The split size is controlled by formula <u>max(minSplitSize, min(maxSplitSize, blockSize))</u>. By default, blockSize=512MB and minSplit < blockSize < maxSplit. <br>
    You should always refer to this formula.<br>
    In the case above, try <u>increase min split size</u> and let each mapper process larger data.<br>
    See <a href="http://go/hadooptuningtips">Hadoop Tuning Tips</a> for further information.<br>

</p>
<h3>Large files/Unsplittable files</h3>
<p>
    This usually happens when the Hadoop job has:
<ul>
    <li>A <u>small</u> number of mappers</li>
    <li><u>Large</u> file size (a few GB's)</li>
    <li><u>Long</u> mapper runtime</li>
</ul>
</p>
<h5>Example</h5>
<p>
<div class="list-group">
    <a class="list-group-item list-group-item-danger" href="#">
        <h4 class="list-group-item-heading">Mapper Input Size</h4>
        <table class="list-group-item-text table table-condensed left-table">
            <thead><tr><th colspan="2">Severity: Critical</th></tr></thead>
            <tbody>
            <tr>
                <td>Number of tasks</td>
                <td>100</td>
            </tr>
            <tr>
                <td>Average task input</td>
                <td>3 GB</td>
            </tr>
            <tr>
                <td>Average task runtime</td>
                <td>32 min 30s</td>
            </tr>
            </tbody>
        </table>
    </a>
</div>
</p>
<h4>Suggestions</h4>
<p>
    The split size is too large. You should tune mapper split size to increase number of mappers and let each mapper process less data.<br>
    <br>
    The input split size is controlled by formula <u>max(minSplitSize, min(maxSplitSize, blockSize))</u>. <br>
    See the previous section for further details. <br>

    In the case above, since mapper input size >> block size and you want to increase mappers, you should decrease min split size close to BlockSize(512MB). <br>

    See <a href="http://go/hadooptuningtips">Hadoop Tuning Tips</a> for further information.
</p>
